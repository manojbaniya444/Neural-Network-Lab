# Neural Netowrks Lab

Here I am exploring different popular and `SOTA` Neural Network Architectures common in the field of `Natural Language Processing`, `Computer Vision` and `MultiModal`.

# What this Repository Contains:

This repository mainly contains `Research Paper Implementation` of popular NLP and CV techniques and try to apply in own use case by training on own data.

### Attention is all you need:

This contains all the concepts from the paper `attention is all you need` paper.

- [Encoder Part Implementation](./Attention%20is%20all%20you%20need%20paper/Encoder%20Part/Transformer%20-%20Encoder.ipynb)

### Nano GPT

Training a character level language modeling with attention mechanism following `GPT 1` like approach.

- [Concept only](./Nano%20GPT/nano-gpt-concepts.ipynb)
- [Nano GPT full Training](./Nano%20GPT/nano-gpt-training.ipynb)

## MicroGrad

Pytorch like Gradient Calculation for backpropagation.

- [Gradient Calculation](./MicroGrad/concepts/micrograd_backpropagation.ipynb)
- [MLP From Scratch](./MicroGrad/concepts/mlp_backpropagation.ipynb)

## MakeMore

Training Neural Network to generate name like text. It is a character level `language modeling`.

- [Character level Bigram Language Modeling](./MakeMore/01_Bigram_Language_Model_NN.ipynb)