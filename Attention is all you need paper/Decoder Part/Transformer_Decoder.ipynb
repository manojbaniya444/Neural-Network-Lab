{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder part of Attention is all you need Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CONFIGURATION\n",
    "\n",
    "NUM_ATTENTION_HEADS = 8\n",
    "D_MODEL = 512\n",
    "DROP_PROB = 0.2\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "FFN_HIDDEN = 2048\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### transpose torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 64, 200]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Conceptual code for scaled dot product\n",
    "\n",
    "key_ = torch.rand(BATCH_SIZE, NUM_ATTENTION_HEADS, MAX_SEQUENCE_LENGTH, D_MODEL // NUM_ATTENTION_HEADS)\n",
    "\n",
    "key_transposed_ = key_.transpose(2, 3) # we want to transpose the dim 2 and dim 3 also we can write transpose(-2, -1)\n",
    "\n",
    "key_.shape, key_transposed_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### masked fill torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones(5, 5)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked = mask.masked_fill(mask == 1, -torch.inf)\n",
    "\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.4976,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.9841,  0.3592,    -inf,    -inf,    -inf],\n",
       "          [ 0.1644,  0.6086,  0.8830,    -inf,    -inf],\n",
       "          [-0.3268, -0.1089, -0.5998, -0.5775,    -inf],\n",
       "          [-0.9658,  0.1995,  1.8666, -0.4802, -1.0530]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2161,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.5256,  1.7169,    -inf,    -inf,    -inf],\n",
       "          [-0.2487,  0.8131,  1.1253,    -inf,    -inf],\n",
       "          [ 0.2340,  0.0390,  0.6614, -0.4125,    -inf],\n",
       "          [ 1.4189, -0.0896, -0.0468, -1.7056,  0.0746]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_ = torch.randn(2, 1, 5, 5)\n",
    "mask_ = torch.full((5, 5), -torch.inf)\n",
    "mask_ = torch.triu(mask_, diagonal=1)\n",
    "\n",
    "attention_scores_masked_ = attention_scores_ + mask_\n",
    "\n",
    "attention_scores_masked_ # 2 batch 1 head 5 seq 5 seq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention:\n",
    "    \n",
    "    Args:\n",
    "       query: Tensor of shape (batch_size, num_heads, seq_len, d_k)\n",
    "       key: Tensor of shape (batch_size, num_heads, seq_len, d_k)\n",
    "       value: Tensor of shape (batch_size, num_heads, seq_len, d_v)\n",
    "       mask: True or False   \n",
    "    Returns:\n",
    "       attention_output: Attention-Weighted values with matmul to value (batch_size, num_heads, seq_len, d_v)\n",
    "       attention_weights: Attention-Weighted Values\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1) # get the dimension of query\n",
    "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # scaled dot product attention with key matrix transposed\n",
    "    if mask is not None:\n",
    "        attention_scores = attention_scores + mask\n",
    "        print(f\"Mask shape: {mask.shape}\")\n",
    "\n",
    "   \n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "    attention_output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    # log\n",
    "    print(f\"Query Key Value shape in Self Attention: {query.shape}, {key.shape}, {value.shape}\")\n",
    "    print(f\"Attention scores shape in Self Attention: {attention_scores.shape}\")\n",
    "    print(f\"Attention weights shape in Self Attention: {attention_weights.shape}\")\n",
    "    print(f\"Attention output shape in Self Attention: {attention_output.shape}\")\n",
    "        \n",
    "    return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: torch.Size([200, 200])\n",
      "Query Key Value shape in Self Attention: torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64])\n",
      "Attention scores shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention weights shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention output shape in Self Attention: torch.Size([32, 8, 200, 64])\n"
     ]
    }
   ],
   "source": [
    "#### testing\n",
    "query = key = value = torch.rand(BATCH_SIZE, NUM_ATTENTION_HEADS, MAX_SEQUENCE_LENGTH, D_MODEL // NUM_ATTENTION_HEADS)\n",
    "\n",
    "mask = torch.full((MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH), float(\"-inf\"))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "attention_output = scaled_dot_product_attention(query, key, value, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((BATCH_SIZE, MAX_SEQUENCE_LENGTH, D_MODEL))\n",
    "y = torch.randn((BATCH_SIZE, MAX_SEQUENCE_LENGTH, D_MODEL))\n",
    "\n",
    "mask = torch.full((MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH), float(\"-inf\"))\n",
    "mask = torch.triu(mask, diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Multi-Head Attention** mechanism is a fundamental part of the Transformer architecture, as described in the paper **\"Attention Is All You Need\"**. This module allows the model to focus on different positions of the input simultaneously, improving its ability to model relationships in sequences.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. Purpose of Multi-Head Attention\n",
    "\n",
    "- The goal of Multi-Head Attention is to allow the model to attend to different parts of the input sequence at different positions and in parallel. \n",
    "- This allows the model to extract more information by applying multiple attention heads, each focusing on different aspects of the input sequence.\n",
    "\n",
    "### 2. Linear Projections\n",
    "\n",
    "- The first step in the multi-head attention mechanism is the linear projection of the input into three different vectors: Queries (\\( Q \\)), Keys (\\( K \\)), and Values (\\( V \\)).\n",
    "- The dimensions of the projections are given by:\n",
    "  $$\n",
    "  Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "  $$\n",
    "  where \\( X \\) is the input matrix, and \\( W_Q \\), \\( W_K \\), \\( W_V \\) are learnable weight matrices.\n",
    "\n",
    "### 3. Scaled Dot-Product Attention\n",
    "\n",
    "- The core operation of attention is the dot-product between the query and the key, scaled by the square root of the dimension of the key $\\sqrt{d_k}$:\n",
    "  $$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "  $$\n",
    "  where \\( d_k \\) is the dimension of the key vectors. The softmax function ensures that the attention weights are normalized.\n",
    "\n",
    "### 4. Multi-Head Attention\n",
    "\n",
    "- Once the attention scores are computed, the attention outputs for each head are concatenated:\n",
    "  $$\n",
    "  \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W_O\n",
    "  $$\n",
    "  where $$\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i)$$, and \\( W_O \\) is the output weight matrix.\n",
    "\n",
    "- The concatenated output is then passed through a final linear layer to project it back to the original dimension $d_{\\text{model}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### view torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 200, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_ = torch.randn((32, 8, 200, 64))\n",
    "Q = query_.view(32, -1, 8, 64 ) # 32 200 8 64\n",
    "Q = Q.transpose(1, 2) # 32 8 200 64\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after linear layer\n",
      "torch.Size([32, 8, 200, 512])\n",
      "After view\n",
      "torch.Size([32, 1600, 8, 64])\n",
      "After transpose\n",
      "torch.Size([32, 8, 1600, 64])\n"
     ]
    }
   ],
   "source": [
    "linear_ = torch.nn.Linear(512, 512)\n",
    "x_ = torch.rand((32, 8, 200, 512)) # query * wq and 32 8 will be broadcasted to match the shape\n",
    "out_ = linear_(x_)\n",
    "\n",
    "print(\"Output after linear layer\")\n",
    "print(out_.shape)\n",
    "\n",
    "out_ = out_.view(32, -1, 8, 64)\n",
    "\n",
    "print(\"After view\")\n",
    "print(out_.shape)\n",
    "\n",
    "out_ = out_.transpose(1, 2)\n",
    "\n",
    "print(\"After transpose\")\n",
    "print(out_.shape) # this is for 8 attenti0n heads so that the shape is 32 batch size 8 heads and 1600 sequence for 8 attention and 64 inner dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Linear layers for query, key, and value projections\n",
    "        # Here directly projecting to the d_model and then later splitting for each attention head for computational efficiency\n",
    "        self.W_Query = nn.Linear(d_model, d_model)\n",
    "        self.W_Key = nn.Linear(d_model, d_model)\n",
    "        self.W_Value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Final linear layer after concatenating heads\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Project and reshape input to (batch_size, num_heads, seq_len, d_k)\n",
    "        Q = self.W_Query(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_Key(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_Value(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        print(f\"Query Key Value Shape in MultiHead Attention after Projecting and Reshaping: {Q.shape} {K.shape} {V.shape}\")\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads and project back to d_model\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        output = self.linear_layer(attention_output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Key Value Shape in MultiHead Attention after Projecting and Reshaping: torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64])\n",
      "Mask shape: torch.Size([200, 200])\n",
      "Query Key Value shape in Self Attention: torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64])\n",
      "Attention scores shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention weights shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention output shape in Self Attention: torch.Size([32, 8, 200, 64])\n",
      "Output of Multi Head Attention: torch.Size([32, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "query = torch.randn(32, 200, 512)  # (batch_size, seq_len, d_model)\n",
    "key = torch.randn(32, 200, 512)\n",
    "value = torch.randn(32, 200, 512)\n",
    "mask = torch.full((MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH), float(\"-inf\"))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "output = mha(query, key, value, mask)\n",
    "\n",
    "print(f\"Output of Multi Head Attention: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model,num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # Masked self attention\n",
    "        print(\"..........................................................................................\")\n",
    "        print(\"\\nDECODER SELF ATTENTION\")\n",
    "        self_attention_output = self.self_attention(x, x, x, tgt_mask)\n",
    "        print(\"\\nADD AND NORM\")\n",
    "        x = self.norm1(x + self.dropout(self_attention_output)) # add and Norm\n",
    "        print(f\"After add and norm: {x.size()}\")\n",
    "        # Cross Attention\n",
    "        print(\"\\nCROSS ATTENTION\")\n",
    "        enc_dec_attention_output = self.enc_dec_attention(x, enc_output, enc_output, src_mask)\n",
    "        print(f\"Output: {enc_dec_attention_output.size()}\")\n",
    "        print(\"\\nADD AND NORM\")\n",
    "        x = self.norm2(x + self.dropout(enc_dec_attention_output)) # add and norm2\n",
    "        print(f\"After add and norm2: {x.size()}\")\n",
    "        \n",
    "        # Feed Forward NN\n",
    "        print(\"\\nFEED FORWARD NN\")\n",
    "        ff_output = self.feed_forward(x)\n",
    "        print(f\"Output of ff_nn: {ff_output.size()}\")\n",
    "        print(\"\\nADD AND NORM\")\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        print(f\"After add and norm3: {x.size()}\")\n",
    "        print(\"................................................................................................\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Positional Encoding: torch.Size([32, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "#### Testing\n",
    "pe = PositionalEncoding(512, 200)\n",
    "x = torch.randn(32, 200, 512)\n",
    "output = pe(x)\n",
    "\n",
    "print(f\"Output of Positional Encoding: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullDecoder(torch.nn.Module):\n",
    "    def __init__(self, d_model=512, num_layers=6, num_heads=8, d_ff=2048, vocab_size=10000, max_len=200):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        print(\"TOKEN EMBEDDING\")\n",
    "        x = self.embedding(x)\n",
    "        print(f\"embedding: {x.size()}\")\n",
    "        print(\"\\nPOSITIONAL EMBEDDING\")\n",
    "        x = self.positional_encoding(x)\n",
    "        print(f\"Input to Decoder: {x.shape}\\n\")\n",
    "        \n",
    "        print(\"**************************DECODER START**************************\")\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"DECODER LAYER: {i+1}\")\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "            print(f\"Output of Decoder {i}: {x.size()}\")\n",
    "        print(\"**************************DECODER FINISH**************************\")\n",
    "        return self.fc_out(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullDecoder(\n",
       "  (embedding): Embedding(10000, 512)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-2): 3 x DecoderLayer(\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (W_Query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_Key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_Value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (enc_dec_attention): MultiHeadAttention(\n",
       "        (W_Query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_Key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_Value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing\n",
    "VOCAB_SIZE = 10000\n",
    "D_MODEL = 512\n",
    "NUM_ATTENTION_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "FFN_HIDDEN = 2048\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "decoder_input = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, MAX_SEQUENCE_LENGTH))\n",
    "encoder_output = torch.randn(BATCH_SIZE, MAX_SEQUENCE_LENGTH, D_MODEL)\n",
    "\n",
    "decoder = FullDecoder(\n",
    "    d_model=D_MODEL,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_ATTENTION_HEADS,\n",
    "    d_ff=FFN_HIDDEN,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_len=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN EMBEDDING\n",
      "embedding: torch.Size([32, 200, 512])\n",
      "\n",
      "POSITIONAL EMBEDDING\n",
      "Input to Decoder: torch.Size([32, 200, 512])\n",
      "\n",
      "**************************DECODER START**************************\n",
      "DECODER LAYER: 1\n",
      "..........................................................................................\n",
      "\n",
      "DECODER SELF ATTENTION\n",
      "Query Key Value Shape in MultiHead Attention after Projecting and Reshaping: torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64])\n",
      "Mask shape: torch.Size([200, 200])\n",
      "Query Key Value shape in Self Attention: torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64])\n",
      "Attention scores shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention weights shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention output shape in Self Attention: torch.Size([32, 8, 200, 64])\n",
      "\n",
      "ADD AND NORM\n",
      "After add and norm: torch.Size([32, 200, 512])\n",
      "\n",
      "CROSS ATTENTION\n",
      "Query Key Value Shape in MultiHead Attention after Projecting and Reshaping: torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64])\n",
      "Mask shape: torch.Size([200, 200])\n",
      "Query Key Value shape in Self Attention: torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64])\n",
      "Attention scores shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention weights shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention output shape in Self Attention: torch.Size([32, 8, 200, 64])\n",
      "Output: torch.Size([32, 200, 512])\n",
      "\n",
      "ADD AND NORM\n",
      "After add and norm2: torch.Size([32, 200, 512])\n",
      "\n",
      "FEED FORWARD NN\n",
      "Output of ff_nn: torch.Size([32, 200, 512])\n",
      "\n",
      "ADD AND NORM\n",
      "After add and norm3: torch.Size([32, 200, 512])\n",
      "................................................................................................\n",
      "Output of Decoder 0: torch.Size([32, 200, 512])\n",
      "DECODER LAYER: 2\n",
      "..........................................................................................\n",
      "\n",
      "DECODER SELF ATTENTION\n",
      "Query Key Value Shape in MultiHead Attention after Projecting and Reshaping: torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64])\n",
      "Mask shape: torch.Size([200, 200])\n",
      "Query Key Value shape in Self Attention: torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64])\n",
      "Attention scores shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention weights shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention output shape in Self Attention: torch.Size([32, 8, 200, 64])\n",
      "\n",
      "ADD AND NORM\n",
      "After add and norm: torch.Size([32, 200, 512])\n",
      "\n",
      "CROSS ATTENTION\n",
      "Query Key Value Shape in MultiHead Attention after Projecting and Reshaping: torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64])\n",
      "Mask shape: torch.Size([200, 200])\n",
      "Query Key Value shape in Self Attention: torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64])\n",
      "Attention scores shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention weights shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention output shape in Self Attention: torch.Size([32, 8, 200, 64])\n",
      "Output: torch.Size([32, 200, 512])\n",
      "\n",
      "ADD AND NORM\n",
      "After add and norm2: torch.Size([32, 200, 512])\n",
      "\n",
      "FEED FORWARD NN\n",
      "Output of ff_nn: torch.Size([32, 200, 512])\n",
      "\n",
      "ADD AND NORM\n",
      "After add and norm3: torch.Size([32, 200, 512])\n",
      "................................................................................................\n",
      "Output of Decoder 1: torch.Size([32, 200, 512])\n",
      "DECODER LAYER: 3\n",
      "..........................................................................................\n",
      "\n",
      "DECODER SELF ATTENTION\n",
      "Query Key Value Shape in MultiHead Attention after Projecting and Reshaping: torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64])\n",
      "Mask shape: torch.Size([200, 200])\n",
      "Query Key Value shape in Self Attention: torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64])\n",
      "Attention scores shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention weights shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention output shape in Self Attention: torch.Size([32, 8, 200, 64])\n",
      "\n",
      "ADD AND NORM\n",
      "After add and norm: torch.Size([32, 200, 512])\n",
      "\n",
      "CROSS ATTENTION\n",
      "Query Key Value Shape in MultiHead Attention after Projecting and Reshaping: torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64]) torch.Size([32, 8, 200, 64])\n",
      "Mask shape: torch.Size([200, 200])\n",
      "Query Key Value shape in Self Attention: torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64]), torch.Size([32, 8, 200, 64])\n",
      "Attention scores shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention weights shape in Self Attention: torch.Size([32, 8, 200, 200])\n",
      "Attention output shape in Self Attention: torch.Size([32, 8, 200, 64])\n",
      "Output: torch.Size([32, 200, 512])\n",
      "\n",
      "ADD AND NORM\n",
      "After add and norm2: torch.Size([32, 200, 512])\n",
      "\n",
      "FEED FORWARD NN\n",
      "Output of ff_nn: torch.Size([32, 200, 512])\n",
      "\n",
      "ADD AND NORM\n",
      "After add and norm3: torch.Size([32, 200, 512])\n",
      "................................................................................................\n",
      "Output of Decoder 2: torch.Size([32, 200, 512])\n",
      "**************************DECODER FINISH**************************\n",
      "Output of After Final Layer: torch.Size([32, 200, 10000])\n"
     ]
    }
   ],
   "source": [
    "output = decoder(decoder_input,encoder_output, mask, mask)\n",
    "\n",
    "print(f\"Output of After Final Layer: {output.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
